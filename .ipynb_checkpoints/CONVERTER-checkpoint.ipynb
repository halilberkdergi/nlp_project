{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "novel-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
    "from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer\n",
    "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "import urllib,re,sys,csv,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "administrative-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(subtitle_file,excel_name,sheet_name):\n",
    "    \n",
    "    #read input file\n",
    "    file = open(subtitle_file)\n",
    "    SENTENCES_COUNT =0         \n",
    "    data = file.read()\n",
    "\n",
    "    CoList = data.split('\\n')\n",
    "\n",
    "    for i in CoList:\n",
    "        if i:        \n",
    "            SENTENCES_COUNT += 1\n",
    "\n",
    "    data = data.replace('\\n', ' . ')\n",
    "    #close the input file\n",
    "    file.close()\n",
    "\n",
    "    ############\n",
    "    #open the input file in write mode\n",
    "    fin = open(subtitle_file, \"wt\")\n",
    "    #overrite the input file with the resulting data\n",
    "    fin.write(data)\n",
    "    #close the file\n",
    "    fin.close()\n",
    "\n",
    "    ###### read again####\n",
    "    document = open(subtitle_file).read()\n",
    "\n",
    "    ####PANDAS####\n",
    "    df = pd.read_excel(excel_name,header=0,sheet_name=sheet_name,index_col=0)\n",
    "    df.index.name = None\n",
    "    df =df.fillna(0)\n",
    "    df= df.astype(int)\n",
    "    \n",
    "\n",
    "    #####PYSUMMM#####\n",
    "    \n",
    "    auto_abstractor = AutoAbstractor() # Object of automatic summarization.\n",
    "    auto_abstractor.tokenizable_doc = SimpleTokenizer() # Set tokenizer.\n",
    "    auto_abstractor.delimiter_list = [\".\"] # Set delimiter for making a list of sentence.\n",
    "    abstractable_doc = TopNRankAbstractor()# Object of abstracting and filtering document.\n",
    "    \n",
    "    for n in range(1,SENTENCES_COUNT+1):\n",
    "        abstractable_doc.set_top_n(n) # set n = 120 value\n",
    "        result_dict = auto_abstractor.summarize(document, abstractable_doc) # Summarize document.\n",
    "        #print(n )\n",
    "\n",
    "        for i in range(n):\n",
    "            a=result_dict[\"scoring_data\"][i][0]+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['pysum'][a]==0:\n",
    "                df['pysum'][a] = n\n",
    "\n",
    "    #####SUMY#########\n",
    "    LANGUAGE = \"english\"\n",
    "    parser = PlaintextParser.from_file(subtitle_file, Tokenizer(LANGUAGE))\n",
    "    summarizer_lex = LexRankSummarizer()\n",
    "\n",
    "    summarizer_luhn = LuhnSummarizer()\n",
    "\n",
    "    summarizer_lsa = LsaSummarizer()\n",
    "\n",
    "    summarizer_textrank = TextRankSummarizer()\n",
    "\n",
    "    for n in range(1,SENTENCES_COUNT):\n",
    "        summary_lex = summarizer_lex(parser.document,n)\n",
    "        summary_luhn = summarizer_luhn(parser.document,n)\n",
    "        summary_lsa = summarizer_lsa(parser.document,n)\n",
    "        summary_textrank = summarizer_textrank(parser.document,n)\n",
    "        for sentence in summary_lex:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['lexrank'][a]==0:\n",
    "                df['lexrank'][a] = n\n",
    "        for sentence in summary_luhn:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['luhn'][a]==0:\n",
    "                df['luhn'][a] = n\n",
    "        for sentence in summary_lsa:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['lsa'][a]==0:\n",
    "                df['lsa'][a] = n\n",
    "        for sentence in summary_textrank:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['textrank'][a]==0:\n",
    "                df['textrank'][a] = n\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surface-puzzle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pysum</th>\n",
       "      <th>lexrank</th>\n",
       "      <th>luhn</th>\n",
       "      <th>lsa</th>\n",
       "      <th>textrank</th>\n",
       "      <th>x1.</th>\n",
       "      <th>x1.3</th>\n",
       "      <th>x1.6</th>\n",
       "      <th>x1.9</th>\n",
       "      <th>x2.2</th>\n",
       "      <th>x2.5</th>\n",
       "      <th>nadir_kelime</th>\n",
       "      <th>konu_ile_alakalı</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>63</td>\n",
       "      <td>109</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>102</td>\n",
       "      <td>24</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>71</td>\n",
       "      <td>83</td>\n",
       "      <td>27</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>97</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>106</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pysum  lexrank  luhn  lsa  textrank  x1.  x1.3  x1.6  x1.9  x2.2  x2.5  \\\n",
       "1        8       78    63  109        88    1     1     1     1     1     1   \n",
       "2       11       64   102   24        83    1     1     1     1     1     1   \n",
       "3       10       71    83   27       106    1     1     1     1     1     0   \n",
       "4        7      110     7   18        12    1     1     1     1     1     1   \n",
       "5        6        2     3   17         4    1     1     1     1     1     0   \n",
       "..     ...      ...   ...  ...       ...  ...   ...   ...   ...   ...   ...   \n",
       "116     99       84    97  115         0    1     1     1     1     1     1   \n",
       "117     49       25    21   20        43    1     1     1     1     1     1   \n",
       "118     39       31    32   16        16    1     1     1     1     1     0   \n",
       "119     24       38    33    1         1    1     1     1     1     1     0   \n",
       "120     66       80    73  106        66    0     0     0     0     0     0   \n",
       "\n",
       "     nadir_kelime  konu_ile_alakalı  \n",
       "1               0                 0  \n",
       "2               0                 0  \n",
       "3               0                 0  \n",
       "4               0                 0  \n",
       "5               0                 0  \n",
       "..            ...               ...  \n",
       "116             0                 0  \n",
       "117             0                 0  \n",
       "118             0                 0  \n",
       "119             0                 0  \n",
       "120             0                 0  \n",
       "\n",
       "[120 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(subtitle_file='Sub_1.txt',excel_name='data.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-thesaurus",
   "metadata": {},
   "source": [
    "# Nadir geçen kelime bulma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-carry",
   "metadata": {},
   "source": [
    "###### Metinde 1 defa geçen kelimeleri bulma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "informed-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "doc = open('Sub_1.txt').read()\n",
    "tokens = nltk.word_tokenize(doc)\n",
    "fdist = FreqDist(tokens)\n",
    "#len(list(filter(lambda x : x[1]==1, fdist.items())))\n",
    "liste= list(filter(lambda x : x[1]==1, fdist.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "promising-muscle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-prison",
   "metadata": {},
   "source": [
    "##### google ngram viewer sitesinden kelimelerin 2019 yılındaki dildeki unigram olasılıklarını çekiyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "delayed-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora={'eng_2019':26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "danish-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractCleanTerms(regExpression, filterTerms, fullText):\n",
    "\n",
    "    foundSections=re.findall(regExpression, fullText)\n",
    "\n",
    "    for index in range(len(foundSections)):\n",
    "        for filterTerm in filterTerms:\n",
    "            foundSections[index] = re.sub(filterTerm, '', foundSections[index])\n",
    "\n",
    "    return foundSections\n",
    "\n",
    "def getNgrams(query, corpus, startYear, endYear, smoothing):\n",
    "    urlquery = urllib.parse.quote_plus(query, safe='\"')\n",
    "    corpusNumber=corpora[corpus]\n",
    "    url = 'http://books.google.com/ngrams/graph?content=%s&year_start=%d&year_end=%d&corpus=%d&smoothing=%d&share='%(urlquery,startYear,endYear,corpusNumber,smoothing)\n",
    "   #response = urllib.request.urlopen( url ).rea\n",
    "    with urllib.request.urlopen(url) as res:\n",
    "        response = res.read().decode(\"utf-8\")\n",
    "    \n",
    "    timeseries = extractCleanTerms(\"\\\"timeseries\\\": \\[.*?\\]\",[\"\\\"timeseries\\\": \\[\",\"\\]\"],response)\n",
    "    termsSearched = extractCleanTerms(\"\\{\\\"ngram\\\": \\\".*?\\\"\",[\"\\{\\\"ngram\\\": \\\"\",\"\\\"\"],response)\n",
    "    \n",
    "    termsToTimeseries = {}\n",
    "    for index in range(len(termsSearched)):\n",
    "        termsToTimeseries[termsSearched[index]] = [float(time) for time in timeseries[index].split(\",\")]\n",
    "\n",
    "    return url, urlquery, termsToTimeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-harbor",
   "metadata": {},
   "source": [
    "Tek seferde maksimum 12 kelime aratabiliyoruz. Bu nedenle,altyazı metninde 1 defa geçmiş olan kelimeleri çıkarttıktan sonra, 12'şerli paketler halinde query olarak google ngram viewer'a gönderiyoruz. Bu istek paketinden geriye sözcük ve olasılıkları ahatar-değer ikilisi olarak elde ediyoruz. Olasılıkları %0.0000003 altında olan kelimelerin anahtar değerini, yani sözcükleri seçiyoruz. Böylece nadir geçen kelimeleri bulmuş oluyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "plastic-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['chou', 'ezio', 'auditore']\n",
      "24\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d']\n",
      "36\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d']\n",
      "48\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese']\n",
      "60\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese']\n",
      "72\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing']\n",
      "84\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing']\n",
      "96\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing']\n",
      "108\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "120\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "132\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "144\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "156\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "168\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "180\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating']\n",
      "192\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches']\n",
      "204\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand']\n",
      "216\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand', 'fiddly']\n",
      "228\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand', 'fiddly']\n",
      "240\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand', 'fiddly', 'shell - shocked']\n",
      "252\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand', 'fiddly', 'shell - shocked']\n",
      "264\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand', 'fiddly', 'shell - shocked']\n",
      "271\n",
      "['chou', 'ezio', 'auditore', 'macguffin', '2 - d', 'brushstrokes', 'chinese', 'front - facing', 'assassinating', 'glitches', 'swordplay', 'easy - to - understand', 'fiddly', 'shell - shocked', 'eddie']\n"
     ]
    }
   ],
   "source": [
    "call = ''\n",
    "dictlist =[]\n",
    "for n in range(len(liste)):\n",
    "    if ((n%12==0) & (n!=0)) or n==(len(liste)-1):\n",
    "        print(n)      \n",
    "                \n",
    "        arguments = call.split()\n",
    "        query = ' '.join([arg for arg in arguments] )\n",
    "        url, urlquery,data = getNgrams(query, corpus='eng_2019', startYear=2018, endYear=2019, smoothing=3)\n",
    "        \n",
    "\n",
    "        for key, value in data.items():\n",
    "            if value[1]<0.0000003:\n",
    "                dictlist.append(key)\n",
    "        \n",
    "        print(dictlist)\n",
    "        call=liste[n][0] \n",
    "        \n",
    "        \n",
    "    else:\n",
    "       \n",
    "        call+=', '+ liste[n][0]\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "textile-safety",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-secretariat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
