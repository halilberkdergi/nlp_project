{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "novel-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
    "from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer\n",
    "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "administrative-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(subtitle_file,excel_name,sheet_name):\n",
    "    \n",
    "    #read input file\n",
    "    file = open(subtitle_file)\n",
    "    SENTENCES_COUNT =0         \n",
    "    data = file.read()\n",
    "\n",
    "    CoList = data.split('\\n')\n",
    "\n",
    "    for i in CoList:\n",
    "        if i:        \n",
    "            SENTENCES_COUNT += 1\n",
    "\n",
    "    data = data.replace('\\n', ' . ')\n",
    "    #close the input file\n",
    "    file.close()\n",
    "\n",
    "    ############\n",
    "    #open the input file in write mode\n",
    "    fin = open(subtitle_file, \"wt\")\n",
    "    #overrite the input file with the resulting data\n",
    "    fin.write(data)\n",
    "    #close the file\n",
    "    fin.close()\n",
    "\n",
    "    ###### read again####\n",
    "    document = open(subtitle_file).read()\n",
    "\n",
    "    ####PANDAS####\n",
    "    df = pd.read_excel(excel_name,header=0,sheet_name=sheet_name,index_col=0)\n",
    "    df.index.name = None\n",
    "    df =df.fillna(0)\n",
    "    df= df.astype(int)\n",
    "    \n",
    "\n",
    "    #####PYSUMMM#####\n",
    "    \n",
    "    auto_abstractor = AutoAbstractor() # Object of automatic summarization.\n",
    "    auto_abstractor.tokenizable_doc = SimpleTokenizer() # Set tokenizer.\n",
    "    auto_abstractor.delimiter_list = [\".\"] # Set delimiter for making a list of sentence.\n",
    "    abstractable_doc = TopNRankAbstractor()# Object of abstracting and filtering document.\n",
    "    \n",
    "    for n in range(1,SENTENCES_COUNT+1):\n",
    "        abstractable_doc.set_top_n(n) # set n = 120 value\n",
    "        result_dict = auto_abstractor.summarize(document, abstractable_doc) # Summarize document.\n",
    "        #print(n )\n",
    "\n",
    "        for i in range(n):\n",
    "            a=result_dict[\"scoring_data\"][i][0]+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['pysum'][a]==0:\n",
    "                df['pysum'][a] = n\n",
    "\n",
    "    #####SUMY#########\n",
    "    LANGUAGE = \"english\"\n",
    "    parser = PlaintextParser.from_file(subtitle_file, Tokenizer(LANGUAGE))\n",
    "    summarizer_lex = LexRankSummarizer()\n",
    "\n",
    "    summarizer_luhn = LuhnSummarizer()\n",
    "\n",
    "    summarizer_lsa = LsaSummarizer()\n",
    "\n",
    "    summarizer_textrank = TextRankSummarizer()\n",
    "\n",
    "    for n in range(1,SENTENCES_COUNT):\n",
    "        summary_lex = summarizer_lex(parser.document,n)\n",
    "        summary_luhn = summarizer_luhn(parser.document,n)\n",
    "        summary_lsa = summarizer_lsa(parser.document,n)\n",
    "        summary_textrank = summarizer_textrank(parser.document,n)\n",
    "        for sentence in summary_lex:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['lexrank'][a]==0:\n",
    "                df['lexrank'][a] = n\n",
    "        for sentence in summary_luhn:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['luhn'][a]==0:\n",
    "                df['luhn'][a] = n\n",
    "        for sentence in summary_lsa:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['lsa'][a]==0:\n",
    "                df['lsa'][a] = n\n",
    "        for sentence in summary_textrank:        \n",
    "            a=parser.document.sentences.index(sentence)+1\n",
    "            #print('a ='+str(a))\n",
    "            if df['textrank'][a]==0:\n",
    "                df['textrank'][a] = n\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "surface-puzzle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pysum</th>\n",
       "      <th>lexrank</th>\n",
       "      <th>luhn</th>\n",
       "      <th>lsa</th>\n",
       "      <th>textrank</th>\n",
       "      <th>x1.</th>\n",
       "      <th>x1.3</th>\n",
       "      <th>x1.6</th>\n",
       "      <th>x1.9</th>\n",
       "      <th>x2.2</th>\n",
       "      <th>x2.5</th>\n",
       "      <th>nadir_kelime</th>\n",
       "      <th>konu_ile_alakalÄ±</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>63</td>\n",
       "      <td>109</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>102</td>\n",
       "      <td>24</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>71</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>97</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>49</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>66</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>107</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pysum  lexrank  luhn  lsa  textrank  x1.  x1.3  x1.6  x1.9  x2.2  x2.5  \\\n",
       "1        8       78    63  109        88    1     1     1     1     1     1   \n",
       "2       11       64   102   24        83    1     1     1     1     1     1   \n",
       "3       10       71    83   26       106    1     1     1     1     1     0   \n",
       "4        7      110     7   19        12    1     1     1     1     1     1   \n",
       "5        6        2     3   16         4    1     1     1     1     1     0   \n",
       "..     ...      ...   ...  ...       ...  ...   ...   ...   ...   ...   ...   \n",
       "116     99       84    97  115         0    1     1     1     1     1     1   \n",
       "117     49       25    21   21        43    1     1     1     1     1     1   \n",
       "118     39       31    32   14        16    1     1     1     1     1     0   \n",
       "119     24       38    33    1         1    1     1     1     1     1     0   \n",
       "120     66       80    73  107        66    0     0     0     0     0     0   \n",
       "\n",
       "     nadir_kelime  konu_ile_alakalÄ±  \n",
       "1               0                 0  \n",
       "2               0                 0  \n",
       "3               0                 0  \n",
       "4               0                 0  \n",
       "5               0                 0  \n",
       "..            ...               ...  \n",
       "116             0                 0  \n",
       "117             0                 0  \n",
       "118             0                 0  \n",
       "119             0                 0  \n",
       "120             0                 0  \n",
       "\n",
       "[120 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(subtitle_file='Sub_1.txt',excel_name='data.xlsx',sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-redhead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
